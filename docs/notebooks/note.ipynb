{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FateZ Explain \n",
    "\n",
    "This notebook demonstrate how to utilize explanatory features of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import fatez.lib as lib\n",
    "import fatez.model as model\n",
    "import fatez.model.mlp as mlp\n",
    "import fatez.model.bert as bert\n",
    "import fatez.model.gat as gat\n",
    "import fatez.model.sparse_gat as sgat\n",
    "import fatez.process.explainer as explainer\n",
    "import fatez.process.fine_tuner as fine_tuner\n",
    "import fatez.process.pre_trainer as pre_trainer\n",
    "\n",
    "# Ignoring warnings because of using LazyLinear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some fake data and build model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 10              # Equivalent to total gene number\n",
    "top_k = 4           # Equivalent to TF number\n",
    "n_feature = 3       # Feature matrix dimmension\n",
    "n_sample = 10       # Fake samples to make\n",
    "batch_size = 1      # Batch size\n",
    "n_class = 4         # Class number\n",
    "n_bin = 100         # Depreciated\n",
    "masker_ratio = 0.5  # Masking ratio before data input to BERT Encoder\n",
    "\n",
    "# Params for GAT\n",
    "gat_param = {\n",
    "    'd_model': n_feature,\n",
    "    'en_dim': 8,\n",
    "    'n_hidden': 4,\n",
    "    'nhead': 2,\n",
    "    'device':'cpu',\n",
    "    'dtype': torch.float32,\n",
    "}\n",
    "\n",
    "# Params for BERT\n",
    "# Need to make sure d_model is divisible by nhead\n",
    "bert_encoder_param = {\n",
    "    'd_model': gat_param['en_dim'],\n",
    "    'n_layer': 6,\n",
    "    'nhead': 8,\n",
    "    'dim_feedforward': gat_param['en_dim'],\n",
    "    'dtype': torch.float32,\n",
    "}\n",
    "\n",
    "# Generate Fake data\n",
    "dataset = lib.FateZ_Dataset(\n",
    "    samples = [\n",
    "        [\n",
    "            torch.randn(k, gat_param['d_model'], dtype = torch.float32),\n",
    "            torch.randn(top_k, k, dtype = torch.float32)\n",
    "        ] for i in range(n_sample)\n",
    "    ],\n",
    "    labels = torch.empty(n_sample, dtype = torch.long).random_(n_class)\n",
    ")\n",
    "# Make datalaoder\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Build Models\n",
    "gat_model = gat.GAT(**gat_param)\n",
    "masker = model.Masker(ratio = masker_ratio)\n",
    "bert_encoder = bert.Encoder(**bert_encoder_param)\n",
    "\n",
    "print('Fake gene num:', k)\n",
    "print('Fake TF num:', top_k)\n",
    "print('Fake Sample Number:', n_sample)\n",
    "print('Batch Size:', batch_size)\n",
    "print('Class Number:', n_class, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform pre-training with no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pre_training = pre_trainer.Model(\n",
    "    gat = gat_model,\n",
    "    masker = masker,\n",
    "    bin_pro = model.Binning_Process(n_bin = n_bin),\n",
    "    bert_model = bert.Pre_Train_Model(\n",
    "        bert_encoder, n_bin = n_bin, n_dim = gat_model.d_model\n",
    "    )\n",
    ")\n",
    "\n",
    "for input, _ in train_dataloader:\n",
    "    output = pre_training(input[0], input[1])\n",
    "    nn.L1Loss()(\n",
    "        output,\n",
    "        torch.split(input[0], output.shape[1] , dim = 1)[0]\n",
    "    ).backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can go for fine tuning part with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning = fine_tuner.Model(\n",
    "    gat = gat_model,\n",
    "    bin_pro = model.Binning_Process(n_bin = n_bin),\n",
    "    bert_model = bert.Fine_Tune_Model(\n",
    "        bert_encoder,\n",
    "        n_hidden = 2,\n",
    "        n_class = n_class\n",
    "    )\n",
    ")\n",
    "\n",
    "for input, label in train_dataloader:\n",
    "    output = fine_tuning(input[0], input[1])\n",
    "    nn.CrossEntropyLoss()(output, label).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain Fine Tuning model in general. \n",
    "\n",
    "Note: to make overall conclusion on the contribution of a specific gene, we would need to sum up importance values for every feature dimension (RNA-count, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get background data\n",
    "background_data = [a for a, _ in DataLoader(dataset, batch_size = n_sample)][0]\n",
    "explain = explainer.Gradient(fine_tuning, background_data)\n",
    "\n",
    "# vars can be used to estimate how accurate the explanation would be: lower the better\n",
    "shap_values, vars = explain.shap_values(input, return_variances = True)\n",
    "print(f'Explaining {len(shap_values)} classes.')\n",
    "\n",
    "# Having 2 inputs\n",
    "assert len(shap_values[0]) == 2\n",
    "\n",
    "# Only the feat mat explanation should be working\n",
    "print(shap_values[0][0].shape)\n",
    "\n",
    "# Only the adj mat explanation should NOT be working since lacking gradient\n",
    "print(shap_values[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the BERT part for analyzing importances of TFs only.\n",
    "\n",
    "Note: similarly, we would want to sum up values across embed dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We also should accumulate gat_out for every trained input.\n",
    "# Here I just make 1 gat_out for example\n",
    "gat_out = fine_tuning.get_gat_output(input[0], input[1])\n",
    "explain = explainer.Gradient(fine_tuning.bert_model, gat_out)\n",
    "\n",
    "shap_values, vars = explain.shap_values(gat_out, return_variances = True)\n",
    "print(f'Explaining {len(shap_values)} classes.')\n",
    "\n",
    "# Now we only have one input.\n",
    "print(shap_values[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the GAT for analyzing GRP importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "att_explain, last_explain = gat_model.explain(input[0][0], input[1][0])\n",
    "\n",
    "# There is attention head\n",
    "if att_explain is not None:\n",
    "    grp_explain = torch.matmul(last_explain, att_explain)\n",
    "\n",
    "# No attention head\n",
    "else:\n",
    "    grp_explain = last_explain\n",
    "    \n",
    "print(grp_explain.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
