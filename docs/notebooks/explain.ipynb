{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FateZ Explain \n",
    "\n",
    "This notebook demonstrate how to utilize explanatory methods of FateZ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import fatez.test as test\n",
    "import fatez.model as model\n",
    "import fatez.tool.JSON as JSON\n",
    "import fatez.process.explainer as explainer\n",
    "import fatez.process.fine_tuner as fine_tuner\n",
    "import fatez.process.pre_trainer as pre_trainer\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "# Ignoring warnings because of using LazyLinear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and make some fake data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'k': 10,              # Equivalent to total gene number\n",
    "    'top_k': 4,           # Equivalent to TF number\n",
    "    'n_features': 3,      # Feature matrix dimmension\n",
    "    'n_sample': 10,       # Fake samples to make\n",
    "    'batch_size': 1,      # Batch size\n",
    "    'n_class': 4,         # Class number\n",
    "}\n",
    "\n",
    "# Load built-in config file\n",
    "config = JSON.decode(resource_filename(\n",
    "        __name__, '../../fatez/data/config/test_config.json'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Adjust parametes according to data dims\n",
    "config['gat']['params']['d_model'] = params['n_features']\n",
    "config['fine_tuner']['n_class'] = params['n_class']\n",
    "factory_kwargs = {'device': 'cpu', 'dtype': torch.float32,}\n",
    "\n",
    "# Generate Fake data\n",
    "faker = test.Faker(model_config = config, **params)\n",
    "train_dataloader = faker.make_data_loader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform pre-training with no label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here trainer's $n\\_dim\\_adj$ is set to None, and the model is NOT reconstructing the adjacency matrices, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Trainer total loss:1.1127713918685913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "pt_loss = trainer.train(train_dataloader)\n",
    "print(f'Pre-Trainer total loss:{pt_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of pre-training with reconstructing adjacency matrices as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Trainer total loss:1.924447774887085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config['pre_trainer']['n_dim_adj'] = params['k']\n",
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "pt_loss = trainer.train(train_dataloader)\n",
    "print(f'Pre-Trainer total loss:{pt_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can go for fine tuning part with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tuner = fine_tuner.Tuner(\n",
    "    gat = trainer.model.gat,\n",
    "    encoder = trainer.model.bert_model.encoder,\n",
    "    rep_embedder = trainer.model.bert_model.rep_embedder,\n",
    "    **config['fine_tuner'],\n",
    "    **factory_kwargs,\n",
    ")\n",
    "\n",
    "for input, label in train_dataloader:\n",
    "    output = tuner.model(input[0], input[1])\n",
    "    torch.nn.CrossEntropyLoss()(output, label).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain Fine Tuning model in general. \n",
    "\n",
    "Note: to make overall conclusion on the contribution of a specific gene, we would need to sum up importance values for every feature dimension (RNA-count, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining 4 classes.\n",
      "Each class has explain in shape of (1, 10, 3).\n"
     ]
    }
   ],
   "source": [
    "# Get background data\n",
    "background_data = [a for a, _ in DataLoader(train_dataloader.dataset, batch_size = params['n_sample'])][0]\n",
    "explain = explainer.Gradient(tuner.model, background_data)\n",
    "\n",
    "# vars can be used to estimate how accurate the explanation would be: lower the better\n",
    "gene_shap_values, vars = explain.shap_values(input, return_variances = True)\n",
    "print(f'Explaining {len(gene_shap_values)} classes.')\n",
    "\n",
    "# Having 2 inputs\n",
    "assert len(gene_shap_values[0]) == 2\n",
    "\n",
    "# Only the feat mat explanation should be working\n",
    "print(f'Each class has explain in shape of {gene_shap_values[0][0].shape}.')\n",
    "\n",
    "# The adj mat explanation should NOT be working since lacking gradient\n",
    "# print(gene_shap_values[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the BERT part for analyzing importances of TFs only.\n",
    "\n",
    "Note: similarly, we would want to sum up values across embed dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining 4 classes.\n",
      "Each class has explain in shape of (1, 4, 3).\n"
     ]
    }
   ],
   "source": [
    "# We also should accumulate gat_out for every trained input.\n",
    "# Here I just make 1 gat_out for example\n",
    "gat_out = tuner.model.get_gat_output(input[0], input[1])\n",
    "explain = explainer.Gradient(tuner.model.bert_model, gat_out)\n",
    "\n",
    "regulon_shap_values, vars = explain.shap_values(gat_out, return_variances=True)\n",
    "print(f'Explaining {len(regulon_shap_values)} classes.')\n",
    "print(f'Each class has explain in shape of {regulon_shap_values[0].shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the GAT for analyzing GRP importances.\n",
    "\n",
    "The grp_explain here is purely based on the GAT attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "grp_explain = tuner.model.gat.explain(input[0][0], input[1][0])\n",
    "print(grp_explain.shape)\n",
    "\n",
    "# Or we can feed in matrices with ones to extract attention weights.\n",
    "grp_explain = tuner.model.gat.explain(\n",
    "    torch.ones_like(input[0][0]), torch.ones_like(input[1][0])\n",
    ")\n",
    "print(grp_explain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the importance values of each gene or TF regulon inferred from the calculated shapley values above would be sufficent to obtain importances of each GRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Sum up shapley values of each features for every node (gene or TF).\n",
    "regulon_importance_values = regulon_shap_values[0].sum(2)\n",
    "gene_importance_values = gene_shap_values[0][0].sum(2)\n",
    "print(regulon_importance_values.shape)\n",
    "print(gene_importance_values.shape)\n",
    "\n",
    "grp_importance = torch.matmul(\n",
    "    torch.Tensor(regulon_importance_values[0]), grp_explain\n",
    ")\n",
    "grp_importance = torch.matmul(\n",
    "    grp_explain, torch.Tensor(gene_importance_values[0])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
