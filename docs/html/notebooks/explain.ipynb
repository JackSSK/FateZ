{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FateZ Explain \n",
    "\n",
    "This notebook demonstrate how to utilize explanatory methods of FateZ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import fatez.test as test\n",
    "import fatez.model as model\n",
    "import fatez.tool.JSON as JSON\n",
    "import fatez.process as process\n",
    "import fatez.process.fine_tuner as fine_tuner\n",
    "import fatez.process.pre_trainer as pre_trainer\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "# Ignoring warnings because of using LazyLinear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and make some fake data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'n_sample': 10,       # Fake samples to make\n",
    "    'batch_size': 1,      # Batch size\n",
    "}\n",
    "\n",
    "# Load built-in config file\n",
    "config = JSON.decode(resource_filename(\n",
    "        __name__, '../../fatez/data/config/gat_bert_config.json'\n",
    "    )\n",
    ")\n",
    "\n",
    "factory_kwargs = {'device': 'cuda', 'dtype': torch.float32,}\n",
    "\n",
    "# Generate Fake data\n",
    "faker = test.Faker(model_config = config, **params)\n",
    "train_dataloader = faker.make_data_loader()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform pre-training with no label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here trainer's $train\\_adj$ is set to False, and the model is NOT reconstructing the adjacency matrices, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss\n",
      "0   1.523740\n",
      "1   2.056911\n",
      "2   2.464677\n",
      "3   2.160961\n",
      "4   1.769112\n",
      "5   2.518550\n",
      "6   2.565397\n",
      "7   1.717235\n",
      "8   2.797816\n",
      "9   2.683969\n",
      "10  2.225837\n"
     ]
    }
   ],
   "source": [
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "report = trainer.train(train_dataloader, report_batch = True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of pre-training with reconstructing adjacency matrices as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss\n",
      "0   1.895170\n",
      "1   1.933711\n",
      "2   2.200883\n",
      "3   2.626011\n",
      "4   2.680342\n",
      "5   1.341890\n",
      "6   2.156959\n",
      "7   2.802052\n",
      "8   1.489988\n",
      "9   2.411124\n",
      "10  2.153813\n"
     ]
    }
   ],
   "source": [
    "config['pre_trainer']['train_adj'] = True\n",
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "report = trainer.train(train_dataloader, report_batch = True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can go for fine tuning part with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss  ACC\n",
      "0   0.493794  1.0\n",
      "1   0.446725  1.0\n",
      "2   1.043058  0.0\n",
      "3   1.076692  0.0\n",
      "4   1.044668  0.0\n",
      "5   0.486640  1.0\n",
      "6   1.019905  0.0\n",
      "7   0.943047  0.0\n",
      "8   0.482891  1.0\n",
      "9   0.489378  1.0\n",
      "10  0.752680  0.5\n"
     ]
    }
   ],
   "source": [
    "tuner = fine_tuner.Set(config, factory_kwargs, prev_model = trainer.model)\n",
    "report = tuner.train(train_dataloader, report_batch = True,)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain model.\n",
    "\n",
    "Three kinds of explanations are available:\n",
    "1. edge_explain\n",
    "2. regulon_explain\n",
    "3. node_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining 2 classes.\n",
      "Each class has regulon explain in shape of (4, 4).\n",
      "\n",
      "Edge Explain:\n",
      " tensor([[0.0268, 0.0269, 0.0271, 0.0245, 0.0247, 0.0253, 0.0252, 0.0257, 0.0000,\n",
      "         0.0367],\n",
      "        [0.0328, 0.0317, 0.0301, 0.0304, 0.0290, 0.0296, 0.0320, 0.0267, 0.0000,\n",
      "         0.0273],\n",
      "        [0.0282, 0.0253, 0.0250, 0.0250, 0.0254, 0.0252, 0.0271, 0.0290, 0.0000,\n",
      "         0.0285],\n",
      "        [0.0255, 0.0267, 0.0266, 0.0264, 0.0292, 0.0260, 0.0326, 0.0275, 0.0000,\n",
      "         0.0285]]) \n",
      "\n",
      "Reg Explain:\n",
      " tensor([0.0036, 0.0017, 0.0130, 0.0060], dtype=torch.float64) \n",
      "\n",
      "Node Explain:\n",
      " tensor([0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0007, 0.0000,\n",
      "        0.0007], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing edge explain matrix and regulon explain matrix\n",
    "adj_exp = torch.zeros((config['input_sizes']['n_reg'], config['input_sizes']['n_node']))\n",
    "reg_exp = torch.zeros((config['input_sizes']['n_reg'], config['encoder']['d_model']))\n",
    "\n",
    "# Make background data\n",
    "bg = [a for a,_ in DataLoader(train_dataloader.dataset, batch_size = params['n_sample'])][0]\n",
    "# Set explainer through taking input data from pseudo-dataloader\n",
    "explain = tuner.model.make_explainer([a.to(factory_kwargs['device']) for a in bg])\n",
    "\n",
    "for x,_ in train_dataloader:\n",
    "    data = [a.to(factory_kwargs['device']) for a in x]\n",
    "    adj_temp, reg_temp, vars = tuner.model.explain_batch(data, explain)\n",
    "    adj_exp += adj_temp\n",
    "    \n",
    "    print(f'Explaining {len(reg_temp)} classes.')\n",
    "    \n",
    "    # Only the feat mat explanation should be working\n",
    "    print(f'Each class has regulon explain in shape of {reg_temp[0][0].shape}.\\n')\n",
    "\n",
    "    # Only taking explainations for class 0\n",
    "    for exp in reg_temp[0]: reg_exp += abs(exp)\n",
    "    break\n",
    "\n",
    "reg_exp = torch.sum(reg_exp, dim = -1)\n",
    "node_exp = torch.matmul(reg_exp, adj_exp.type(reg_exp.dtype))\n",
    "print('Edge Explain:\\n', adj_exp, '\\n')\n",
    "print('Reg Explain:\\n', reg_exp, '\\n')\n",
    "print('Node Explain:\\n', node_exp, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
