{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FateZ Explain \n",
    "\n",
    "This notebook demonstrate how to utilize explanatory methods of FateZ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import fatez.test as test\n",
    "import fatez.model as model\n",
    "import fatez.tool.JSON as JSON\n",
    "import fatez.process.explainer as explainer\n",
    "import fatez.process.fine_tuner as fine_tuner\n",
    "import fatez.process.pre_trainer as pre_trainer\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "# Ignoring warnings because of using LazyLinear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and make some fake data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'n_sample': 10,       # Fake samples to make\n",
    "    'batch_size': 1,      # Batch size\n",
    "}\n",
    "\n",
    "# Load built-in config file\n",
    "config = JSON.decode(resource_filename(\n",
    "        __name__, '../../fatez/data/config/gat_bert_config.json'\n",
    "    )\n",
    ")\n",
    "\n",
    "factory_kwargs = {'device': 'cuda', 'dtype': torch.float32,}\n",
    "\n",
    "# Generate Fake data\n",
    "faker = test.Faker(model_config = config, **params)\n",
    "train_dataloader = faker.make_data_loader()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform pre-training with no label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here trainer's $n\\_dim\\_adj$ is set to None, and the model is NOT reconstructing the adjacency matrices, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss\n",
      "0   1.371900\n",
      "1   0.768842\n",
      "2   1.256326\n",
      "3   1.237347\n",
      "4   1.134314\n",
      "5   0.577454\n",
      "6   1.105542\n",
      "7   1.238400\n",
      "8   0.952839\n",
      "9   0.915199\n",
      "10  1.055816\n"
     ]
    }
   ],
   "source": [
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "report = trainer.train(train_dataloader, report_batch = True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of pre-training with reconstructing adjacency matrices as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss\n",
      "0   1.816574\n",
      "1   1.730838\n",
      "2   1.741792\n",
      "3   2.077016\n",
      "4   1.784506\n",
      "5   2.210017\n",
      "6   1.868762\n",
      "7   1.616021\n",
      "8   2.049183\n",
      "9   1.998312\n",
      "10  1.889302\n"
     ]
    }
   ],
   "source": [
    "config['pre_trainer']['n_dim_adj'] = config['input_sizes'][0][1]\n",
    "trainer = pre_trainer.Set(config, factory_kwargs)\n",
    "report = trainer.train(train_dataloader, report_batch = True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can go for fine tuning part with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Loss  ACC\n",
      "0   1.393003  0.0\n",
      "1   1.395453  0.0\n",
      "2   1.484483  0.0\n",
      "3   1.469457  0.0\n",
      "4   1.292269  1.0\n",
      "5   1.478473  0.0\n",
      "6   1.389301  0.0\n",
      "7   1.281845  1.0\n",
      "8   1.476806  0.0\n",
      "9   1.279046  1.0\n",
      "10  1.394014  0.3\n"
     ]
    }
   ],
   "source": [
    "tuner = fine_tuner.Set(config, factory_kwargs, prev_model = trainer.model)\n",
    "report = tuner.train(train_dataloader, report_batch = True)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain Fine Tuning model in general. \n",
    "\n",
    "Note: to make overall conclusion on the contribution of a specific gene, we would need to sum up importance values for every feature dimension (RNA-count, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining 4 classes.\n",
      "Each class has explain in shape of (1, 10, 2).\n"
     ]
    }
   ],
   "source": [
    "# Get background data\n",
    "background_data = [a for a, _ in DataLoader(train_dataloader.dataset, batch_size = params['n_sample'])][0]\n",
    "background_data = [a.to(factory_kwargs['device']) for a in background_data]\n",
    "explain = explainer.Gradient(tuner.model, background_data)\n",
    "\n",
    "# vars can be used to estimate how accurate the explanation would be: lower the better\n",
    "for input_data, _ in train_dataloader:\n",
    "    input_data = [i.to(factory_kwargs['device']) for i in input_data]\n",
    "    gene_shap_values, vars = explain.shap_values(input_data, return_variances = True)\n",
    "    break\n",
    "\n",
    "print(f'Explaining {len(gene_shap_values)} classes.')\n",
    "\n",
    "# Having 2 inputs\n",
    "assert len(gene_shap_values[0]) == 2\n",
    "\n",
    "# Only the feat mat explanation should be working\n",
    "print(f'Each class has explain in shape of {gene_shap_values[0][0].shape}.')\n",
    "\n",
    "# The adj mat explanation should NOT be working since lacking gradient\n",
    "# print(gene_shap_values[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the BERT part for analyzing importances of TFs only.\n",
    "\n",
    "Note: similarly, we would want to sum up values across embed dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining 4 classes.\n",
      "Each class has explain in shape of (1, 4, 3).\n"
     ]
    }
   ],
   "source": [
    "# We also should accumulate gat_out for every trained input.\n",
    "# Here I just make 1 gat_out for example\n",
    "gat_out = tuner.model.get_gat_output(input_data[0], input_data[1])\n",
    "explain = explainer.Gradient(tuner.model.bert_model, gat_out)\n",
    "\n",
    "regulon_shap_values, vars = explain.shap_values(gat_out, return_variances=True)\n",
    "print(f'Explaining {len(regulon_shap_values)} classes.')\n",
    "print(f'Each class has explain in shape of {regulon_shap_values[0].shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the GAT for analyzing GRP importances.\n",
    "\n",
    "The grp_explain here is purely based on the GAT attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "grp_explain = tuner.model.gat.explain(input_data[0][0], input_data[1][0])\n",
    "print(grp_explain.shape)\n",
    "\n",
    "# Or we can feed in matrices with ones to extract attention weights.\n",
    "grp_explain = tuner.model.gat.explain(torch.ones_like(input_data[0][0]), torch.ones_like(input_data[1][0]))\n",
    "print(grp_explain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the importance values of each gene or TF regulon inferred from the calculated shapley values above would be sufficent to obtain importances of each GRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Sum up shapley values of each features for every node (gene or TF).\n",
    "regulon_importance_values = regulon_shap_values[0].sum(2)\n",
    "gene_importance_values = gene_shap_values[0][0].sum(2)\n",
    "print(regulon_importance_values.shape)\n",
    "print(gene_importance_values.shape)\n",
    "\n",
    "grp_importance = torch.matmul(torch.Tensor(regulon_importance_values[0]), grp_explain)\n",
    "grp_importance = torch.matmul(grp_explain, torch.Tensor(gene_importance_values[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
