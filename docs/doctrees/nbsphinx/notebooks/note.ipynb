{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FateZ Explain \n",
    "\n",
    "This notebook demonstrate how to utilize explanatory features of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, L1Loss\n",
    "import fatez.lib as lib\n",
    "import fatez.model as model\n",
    "import fatez.model.gat as gat\n",
    "import fatez.model.bert as bert\n",
    "import fatez.process.explainer as explainer\n",
    "import fatez.process.fine_tuner as fine_tuner\n",
    "import fatez.process.pre_trainer as pre_trainer\n",
    "\n",
    "# Ignoring warnings because of using LazyLinear\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some fake data and build model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "k = 10              # Equivalent to total gene number\n",
    "top_k = 4           # Equivalent to TF number\n",
    "n_feature = 3       # Feature matrix dimmension\n",
    "n_sample = 10       # Fake samples to make\n",
    "batch_size = 1      # Batch size\n",
    "n_class = 4         # Class number\n",
    "n_bin = 100         # Depreciated\n",
    "masker_ratio = 0.5  # Masking ratio before data input to BERT Encoder\n",
    "\n",
    "# Params for GAT\n",
    "gat_param = {\n",
    "    'd_model': n_feature,\n",
    "    'en_dim': 8,\n",
    "    'n_hidden': 4,\n",
    "    'nhead': 2,\n",
    "    'device':'cpu',\n",
    "    'dtype': torch.float32,\n",
    "}\n",
    "\n",
    "# Params for BERT\n",
    "# Need to make sure d_model is divisible by nhead\n",
    "bert_encoder_param = {\n",
    "    'd_model': gat_param['en_dim'],\n",
    "    'n_layer': 6,\n",
    "    'nhead': 8,\n",
    "    'dim_feedforward': gat_param['en_dim'],\n",
    "    'dtype': torch.float32,\n",
    "}\n",
    "\n",
    "# Generate Fake data\n",
    "dataset = lib.FateZ_Dataset(\n",
    "    samples = [\n",
    "        [\n",
    "            torch.randn(k, gat_param['d_model'], dtype = torch.float32),\n",
    "            torch.randn(top_k, k, dtype = torch.float32)\n",
    "        ] for i in range(n_sample)\n",
    "    ],\n",
    "    labels = torch.empty(n_sample, dtype = torch.long).random_(n_class)\n",
    ")\n",
    "# Make datalaoder\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Build Models\n",
    "gat_model = gat.GAT(**gat_param)\n",
    "masker = model.Masker(ratio = masker_ratio)\n",
    "bert_encoder = bert.Encoder(**bert_encoder_param)\n",
    "\n",
    "print('Fake gene num:', k)\n",
    "print('Fake TF num:', top_k)\n",
    "print('Fake Sample Number:', n_sample)\n",
    "print('Batch Size:', batch_size)\n",
    "print('Class Number:', n_class, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we perform pre-training with no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pre_training = pre_trainer.Model(\n",
    "    gat = gat_model,\n",
    "    masker = masker,\n",
    "    bin_pro = model.Binning_Process(n_bin = n_bin),\n",
    "    bert_model = bert.Pre_Train_Model(\n",
    "        bert_encoder, n_bin = n_bin, n_dim = gat_model.d_model\n",
    "    )\n",
    ")\n",
    "\n",
    "for input, _ in train_dataloader:\n",
    "    output = pre_training(input[0], input[1])\n",
    "    L1Loss()(\n",
    "        output, torch.split(input[0], output.shape[1] , dim=1)[0]\n",
    "    ).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can go for fine tuning part with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fine_tuning = fine_tuner.Model(\n",
    "    gat = gat_model,\n",
    "    bin_pro = model.Binning_Process(n_bin = n_bin),\n",
    "    bert_model = bert.Fine_Tune_Model(\n",
    "        bert_encoder, n_hidden = 2, n_class = n_class\n",
    "    )\n",
    ")\n",
    "\n",
    "for input, label in train_dataloader:\n",
    "    output = fine_tuning(input[0], input[1])\n",
    "    CrossEntropyLoss()(output, label).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain Fine Tuning model in general. \n",
    "\n",
    "Note: to make overall conclusion on the contribution of a specific gene, we would need to sum up importance values for every feature dimension (RNA-count, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get background data\n",
    "background_data = [a for a, _ in DataLoader(dataset, batch_size = n_sample)][0]\n",
    "explain = explainer.Gradient(fine_tuning, background_data)\n",
    "\n",
    "# vars can be used to estimate how accurate the explanation would be: lower the better\n",
    "gene_shap_values, vars = explain.shap_values(input, return_variances = True)\n",
    "print(f'Explaining {len(gene_shap_values)} classes.')\n",
    "\n",
    "# Having 2 inputs\n",
    "assert len(gene_shap_values[0]) == 2\n",
    "\n",
    "# Only the feat mat explanation should be working\n",
    "print(gene_shap_values[0][0].shape)\n",
    "\n",
    "# Only the adj mat explanation should NOT be working since lacking gradient\n",
    "print(gene_shap_values[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the BERT part for analyzing importances of TFs only.\n",
    "\n",
    "Note: similarly, we would want to sum up values across embed dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We also should accumulate gat_out for every trained input.\n",
    "# Here I just make 1 gat_out for example\n",
    "gat_out = fine_tuning.get_gat_output(input[0], input[1])\n",
    "explain = explainer.Gradient(fine_tuning.bert_model, gat_out)\n",
    "\n",
    "regulon_shap_values, vars = explain.shap_values(gat_out, return_variances=True)\n",
    "print(f'Explaining {len(regulon_shap_values)} classes.')\n",
    "\n",
    "# Now we only have one input.\n",
    "print(regulon_shap_values[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explain the GAT for analyzing GRP importances.\n",
    "\n",
    "The grp_explain here is purely based on the GAT attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "grp_explain = gat_model.explain(input[0][0], input[1][0])\n",
    "print(grp_explain.shape)\n",
    "\n",
    "# Or we can feed in matrices with ones to extract attention weights.\n",
    "grp_explain = gat_model.explain(\n",
    "    torch.ones_like(input[0][0]), torch.ones_like(input[1][0])\n",
    ")\n",
    "print(grp_explain.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the importance values of each gene or TF regulon inferred from the calculated shapley values above would be sufficent to obtain importances of each GRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sum up shapley values of each features for every node (gene or TF).\n",
    "regulon_importance_values = regulon_shap_values[0].sum(2)\n",
    "gene_importance_values = gene_shap_values[0][0].sum(2)\n",
    "print(regulon_importance_values.shape)\n",
    "print(gene_importance_values.shape)\n",
    "\n",
    "grp_importance = torch.matmul(\n",
    "    torch.Tensor(regulon_importance_values[0]), grp_explain\n",
    ")\n",
    "grp_importance = torch.matmul(\n",
    "    grp_explain, torch.Tensor(gene_importance_values[0])\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
